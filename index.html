<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="In-Depth and In-Breadth: Pre-training Multimodal Language Models Customized for Comprehensive Chart Understanding.">
  <meta name="keywords" content="ChartScope, ChartDQA, Chart Understanding, Chart Data generation, Chart QA, Multimodal Language Models, Pre-training, Data-driven QAs, JSON-only QAs">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>In-Depth and In-Breadth: Pre-training Multimodal Language Models Customized for Comprehensive Chart Understanding</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!-- <link rel="icon" href="./static/images/logo.png"> -->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://sites.google.com/view/wancyuanfan">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://davidhalladay.github.io/mmfactory_demo/">
            MMFactory
          </a>
          <a class="navbar-item" href="https://davidhalladay.github.io/m3t_demo/">
            TAM-VT
          </a>
          <a class="navbar-item" href="https://sites.google.com/view/wancyuanfan/projects/frido">
            Frido
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">
            <!-- <img src="./static/images/logo.png" alt="Logo" style="width: 60px; height: 60px; vertical-align: middle;">  -->
            In-Depth and In-Breadth: 
          </h1>
          <h1 class="title is-2 publication-title">Pre-training Multimodal Language Models Customized for Comprehensive Chart Understanding</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://sites.google.com/view/wancyuanfan">Wan-Cyuan Fan</a><sup>1,3</sup>,</span>
            <span class="author-block">
              <a href="https://www.microsoft.com/en-us/research/people/yenche/">Yen-Chun Chen</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=cOPQtYgAAAAJ&hl=zh-CN">Mengchen Liu</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://ca.linkedin.com/in/alexander-jacobson-097728265">Alexander Jacobson</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.ca/citations?user=k9TsUVsAAAAJ&hl=en">Lu Yuan</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://www.cs.ubc.ca/~lsigal/">Leonid Sigal</a><sup>1,3,4</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>UBC,</span>
            <span class="author-block"><sup>2</sup>Microsoft,</span>
            <span class="author-block"><sup>3</sup>Vector Institute for AI</span>
            <span class="author-block"><sup>4</sup>CIFAR AI Chair</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2507.14298"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2507.14298"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=RntCHD2azDA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/davidhalladay/ChartScope/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/datasets/ChrisFan/ChartDQA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>HuggingFace</span>
                  </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/teaser.mp4"
                type="video/mp4">
      </video>
    </div>
  </div>
</section> -->


<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
    </div>
  </div>
  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h2 class="title is-3">Abstract</h2>
      <div class="content has-text-justified">
        <p>
          <!-- To address these limitations, w -->
          Recent methods for customizing Large Vision Language Models (LVLMs) for domain-specific tasks have shown promising results in scientific chart comprehension. However, existing approaches face two major limitations: First, they rely on paired data from only a few chart types, limiting generalization to wide range of chart types. Secondly, they lack targeted pre-training for chart-data alignment, which hampers the model's understanding of underlying data. In this paper, we introduce ChartScope, an LVLM optimized for in-depth chart comprehension across diverse chart types. We propose an efficient data generation pipeline that synthesizes paired data for a wide range of chart types, along with a novel Dual-Path training strategy that enabling the model to succinctly capture essential data details while preserving robust reasoning capabilities by incorporating reasoning over the underlying data. Lastly, we establish ChartDQA, a new benchmark for evaluating not only question-answering at different levels but also underlying data understanding. Experimental results demonstrate that ChartScope significantly enhances comprehension on a wide range of chart types.
        </p>

      </div>
    </div>
  </div>
  <div class="hero-body">
    <div class="container">
    </div>
  </div>
</section>


<!-- <section class="section">
  <div class="container is-max-desktop">
    <div class="content has-text-centered">
        <h2 class="title is-3">Video</h2>
        <video id="replay-video"
             controls
             muted
             preload
             playsinline
             width="75%">
        <source src="./static/videos/main.mp4"
                type="video/mp4">
      </video>
    </div>
  </div>
</section> -->


<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">ChartScope</h2>

    <!-- Framework Row -->
    <h2 class="title is-4">Quadratic-scale data generation pipeline</h2>
    <div class="columns is-vcentered">
      <div class="column is-three-fifths">
        <figure class="image is-6by3">
          <img src="./static/images/framework_1.png" alt="Framework Diagram Placeholder">
        </figure>
      </div>
      <div class="column">
        <p>
          Our data generation leverages the promising text content generation and coding abilities of current large language models, e.g., GPT, to generate chart images and data. Specifically, LLMs allow us to synthesize raw data for charts, and then the generated Python script turns the raw data into a chart image. In this way, we can produce image data without accessing costly multimodal LLMs. Unlike previous works that prompt LLMs to iteratively generate CSV data, QAs, and Python script for each chart image -- a process that is costly to massively scale -- our pipeline features parallel code and data generation through shared templates and
          READMEs for consistent definitions and formats across the same chart types.
        </p>
      </div>
    </div>

    <!-- Training Strategy Row -->
    <h2 class="title is-4">Dual-Path training with augmented QAs</h2>
    <div class="columns is-vcentered">
      <div class="column is-two-third">
        <p>
          Unlike generic image understanding, chart image understanding requires the model to not only comprehend the underlying data of the chart but also perform reasoning to obtain the final answers. To enhance the in-depth understanding of the model, we introduce Dual-Path training, built on top of the general chart QA pairs by including two additional augmented QAs: Data-driven QAs and JSON-only QAs. Data-driven QAs are multi-turn QAs that first prompt the model to extract JSON raw data given a chart and then answering the question based on the extracted JSON and chart. JSON-only QAs are instead a pure text QAs. Our goal is to preserve the reasoning ability of LLMs when extending to the chart domain.
        </p>
      </div>
      
      <div class="column">
        <figure class="image is-6by3">
          <img src="./static/images/framework_2.png" alt="Training Strategy Placeholder">
        </figure>
        
      </div>
    </div>

  </div>
</section>


<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3=">ChartDQA</h2>
      
      <!-- Framework Row -->
      <h2 class="title is-4">Quadratic-scale data generation pipeline</h2>
      <div class="columns is-vcentered">
        <div class="column is-three-fifths">
          <figure class="image is-6by3">
            <img src="./static/images/framework_1.png" alt="Framework Diagram Placeholder">
          </figure>
        </div>
        <div class="column">
          <p>
            Our data generation leverages the promising text content generation and coding abilities of current large language models, e.g., GPT, to generate chart images and data. Specifically, LLMs allow us to synthesize raw data for charts, and then the generated Python script turns the raw data into a chart image. In this way, we can produce image data without accessing costly multimodal LLMs. Unlike previous works that prompt LLMs to iteratively generate CSV data, QAs, and Python script for each chart image -- a process that is costly to massively scale -- our pipeline features parallel code and data generation through shared templates and
            READMEs for consistent definitions and formats across the same chart types.
          </p>
        </div>
      </div>

      <!-- Training Strategy Row -->
      <h2 class="title is-4">Dual-Path training with augmented QAs</h2>
      <div class="columns is-vcentered">
        <div class="column is-two-third">
          <p>
            Unlike generic image understanding, chart image understanding requires the model to not only comprehend the underlying data of the chart but also perform reasoning to obtain the final answers. To enhance the in-depth understanding of the model, we introduce Dual-Path training, built on top of the general chart QA pairs by including two additional augmented QAs: Data-driven QAs and JSON-only QAs. Data-driven QAs are multi-turn QAs that first prompt the model to extract JSON raw data given a chart and then answering the question based on the extracted JSON and chart. JSON-only QAs are instead a pure text QAs. Our goal is to preserve the reasoning ability of LLMs when extending to the chart domain.
          </p>
        </div>
        
        <div class="column">
          <figure class="image is-6by3">
            <img src="./static/images/framework_2.png" alt="Training Strategy Placeholder">
          </figure>
          
        </div>
      </div>
      
    </div>
  </div>
  
</section>


<!-- <section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre>
  </div>
</section> -->

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title is-3">Acknowledgements</h2>

        <div class="content has-text-justified">
          <p>
            This work was funded, in part, by the Vector Institute for AI, Canada CIFAR AI Chairs, NSERC Canada Research Chair (CRC), and NSERC Discovery and Discovery Accelerator Supplement Grants. Resources used in preparing this research were provided, in part, by the Province of Ontario, the Government of Canada through CIFAR, <a href="https://www.alliancecan.ca/en">the Digital Research Alliance of Canada</a>, <a href="https://vectorinstitute.ai/\#partners">companies</a> sponsoring the Vector Institute, and Advanced Research Computing at the University of British Columbia. Additional hardware support was provided by John R. Evans Leaders Fund CFI grant and Compute Canada under the Resource Allocation Competition award.
          </p>
        </div>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <!-- <div class="content has-text-centered"> -->
      <!-- <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a> -->
      <!-- <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a> -->
    <!-- </div> -->
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            Website template borrowed from <a
              href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>. 
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <!-- <p>
            Website template borrowed from <a
              href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
          </p> -->
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
